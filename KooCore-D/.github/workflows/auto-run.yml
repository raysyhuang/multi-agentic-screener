name: Auto Run (Daily)

on:
  schedule:
    # Weekdays at 22:00 UTC / 5:00 PM ET (after US market close)
    # Starts first â€” slowest engine (up to 3 hrs), must finish before 9:30 PM ET collection
    - cron: "0 22 * * 1-5"
  workflow_dispatch:

env:
  PYTHON_VERSION: "3.11"
  PYTHONUNBUFFERED: "1"

concurrency:
  group: auto-run
  cancel-in-progress: true

jobs:
  run:
    name: Run main.py all
    runs-on: ubuntu-latest
    timeout-minutes: 180
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Rebuild price database
        env:
          POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
        run: |
          rm -f data/prices.db
          START_DATE=$(date -u -d "400 days ago" +%Y-%m-%d)
          END_DATE=$(date -u +%Y-%m-%d)
          echo "Rebuilding prices.db from ${START_DATE} to ${END_DATE}"
          python main.py db download --start "${START_DATE}" --end "${END_DATE}" --config config/default.yaml --workers 8

      - name: Run full scan
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          POLYGON_API_KEY: ${{ secrets.POLYGON_API_KEY }}
          FMP_API_KEY: ${{ secrets.FMP_API_KEY }}
          ADANOS_API_KEY: ${{ secrets.ADANOS_API_KEY }}
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
          SMTP_USER: ${{ secrets.SMTP_USER }}
          SMTP_PASSWORD: ${{ secrets.SMTP_PASSWORD }}
        run: |
          python main.py all --config config/default.yaml

      - name: Push results to Heroku API
        if: success()
        env:
          KOOCORE_HEROKU_URL: ${{ secrets.KOOCORE_HEROKU_URL }}
          ENGINE_API_KEY: ${{ secrets.ENGINE_API_KEY }}
        run: |
          if [ -z "${KOOCORE_HEROKU_URL}" ]; then
            echo "ERROR: KOOCORE_HEROKU_URL is not configured"
            exit 1
          fi

          HYBRID_FILE=$(ls -t outputs/*/hybrid_analysis_*.json 2>/dev/null | head -1)
          if [ -z "${HYBRID_FILE}" ]; then
            echo "ERROR: No hybrid_analysis_*.json file found under outputs/"
            exit 1
          fi

          RUN_DATE=$(basename "${HYBRID_FILE}" | sed -E 's/^hybrid_analysis_([0-9]{4}-[0-9]{2}-[0-9]{2})\.json$/\1/')
          if [ -z "${RUN_DATE}" ]; then
            echo "ERROR: Could not derive run_date from ${HYBRID_FILE}"
            exit 1
          fi
          echo "RUN_DATE=${RUN_DATE}" >> "$GITHUB_ENV"

          # Pre-ingest data quality guard: block obviously corrupted payloads.
          export HYBRID_FILE RUN_DATE
          python - <<'PY'
          import json
          import os
          import statistics

          with open(os.environ["HYBRID_FILE"], "r", encoding="utf-8") as f:
              hybrid = json.load(f)

          summary = hybrid.get("summary", {}) or {}
          screened = int(
              (summary.get("weekly_top5_count", 0) or 0)
              + (summary.get("pro30_candidates_count", 0) or 0)
              + (summary.get("movers_count", 0) or 0)
          )
          if screened <= 0:
              raise SystemExit("Payload quality check failed: candidates_screened must be > 0")

          top3 = hybrid.get("hybrid_top3", []) or []
          if not top3:
              print("Payload quality warning: hybrid_top3 is empty (allowed)")
          else:
              duplicate_tuples = {}
              entries = []
              for item in top3:
                  ticker = str(item.get("ticker") or "").upper()
                  if not ticker:
                      raise SystemExit("Payload quality check failed: top3 item missing ticker")
                  entry = float(item.get("current_price") or 0)
                  if entry <= 0:
                      raise SystemExit(f"Payload quality check failed: {ticker} has non-positive current_price={entry}")
                  stop = float((item.get("stop") or {}).get("stop_price_for_3pct") or 0)
                  if stop <= 0:
                      raise SystemExit(
                          f"Payload quality check failed: {ticker} missing/invalid stop_price_for_3pct={stop}"
                      )
                  if stop >= entry:
                      raise SystemExit(
                          f"Payload quality check failed: {ticker} stop_price_for_3pct={stop} >= current_price={entry}"
                      )
                  target = float((item.get("target") or {}).get("target_price_for_10pct") or 0)
                  if target <= 0:
                      raise SystemExit(
                          f"Payload quality check failed: {ticker} missing/invalid target_price_for_10pct={target}"
                      )
                  if target <= entry:
                      raise SystemExit(
                          f"Payload quality check failed: {ticker} target_price_for_10pct={target} <= current_price={entry}"
                      )
                  key = (round(entry, 2), round(stop, 2), round(target, 2))
                  duplicate_tuples.setdefault(key, []).append(ticker)
                  entries.append(entry)

              dups = [f"{k}->{v}" for k, v in duplicate_tuples.items() if len(set(v)) > 1]
              if dups:
                  raise SystemExit(f"Payload quality check failed: duplicate entry/target tuples across tickers: {dups[:3]}")

              if len(entries) >= 3:
                  med = statistics.median(entries)
                  mx = max(entries)
                  if med > 0 and mx / med > 8 and med < 40:
                      raise SystemExit(
                          f"Payload quality check failed: entry price outlier ratio too high (max={mx}, median={med})"
                      )

          print(f"Payload quality checks passed for run_date={os.environ['RUN_DATE']}, screened={screened}")
          PY

          echo "Pushing ${HYBRID_FILE} to ${KOOCORE_HEROKU_URL} for run_date=${RUN_DATE}"
          PAYLOAD=$(python - <<'PY'
          import json
          import os

          with open(os.environ["HYBRID_FILE"], "r", encoding="utf-8") as f:
              hybrid = json.load(f)

          print(
              json.dumps(
                  {
                      "hybrid_analysis": hybrid,
                      "run_date": os.environ["RUN_DATE"],
                      "pipeline_duration_s": None,
                  }
              )
          )
          PY
          )

          HTTP_CODE=$(curl -sS -o /tmp/ingest_response.json -w "%{http_code}" -X POST "${KOOCORE_HEROKU_URL}/api/engine/ingest" \
            -H "Content-Type: application/json" \
            -H "X-Engine-Key: ${ENGINE_API_KEY}" \
            -d "${PAYLOAD}" \
            --max-time 30)

          echo "Ingest response HTTP ${HTTP_CODE}: $(cat /tmp/ingest_response.json)"
          if [ "${HTTP_CODE}" -lt 200 ] || [ "${HTTP_CODE}" -ge 300 ]; then
            echo "ERROR: Ingest failed"
            exit 1
          fi

      - name: Verify Heroku results freshness
        if: success()
        env:
          KOOCORE_HEROKU_URL: ${{ secrets.KOOCORE_HEROKU_URL }}
        run: |
          if [ -z "${RUN_DATE}" ]; then
            echo "ERROR: RUN_DATE not set"
            exit 1
          fi

          HTTP_CODE=$(curl -sS -o /tmp/results_response.json -w "%{http_code}" "${KOOCORE_HEROKU_URL}/api/engine/results" --max-time 30)
          echo "Results endpoint HTTP ${HTTP_CODE}: $(cat /tmp/results_response.json)"
          if [ "${HTTP_CODE}" -lt 200 ] || [ "${HTTP_CODE}" -ge 300 ]; then
            echo "ERROR: results endpoint failed"
            exit 1
          fi

          python - <<'PY'
          import json
          import os

          expected = os.environ["RUN_DATE"]
          with open("/tmp/results_response.json", "r", encoding="utf-8") as f:
              payload = json.load(f)

          actual = payload.get("run_date")
          if actual != expected:
              raise SystemExit(f"Freshness check failed: expected run_date={expected}, got {actual}")

          engine_name = payload.get("engine_name")
          if engine_name != "koocore_d":
              raise SystemExit(f"Unexpected engine_name: {engine_name}")

          screened = int(payload.get("candidates_screened", 0) or 0)
          if screened <= 0:
              raise SystemExit(f"Invalid candidates_screened: {screened}")

          picks = payload.get("picks", []) or []
          duplicate_tuples = {}
          for p in picks:
              ticker = str(p.get("ticker") or "")
              entry = float(p.get("entry_price") or 0)
              stop = float(p.get("stop_loss") or 0)
              target = float(p.get("target_price") or 0)
              if entry <= 0 or stop <= 0 or target <= 0:
                  raise SystemExit(
                      f"Freshness/quality check failed: {ticker} has non-positive risk fields "
                      f"(entry={entry}, stop={stop}, target={target})"
                  )
              if not (stop < entry < target):
                  raise SystemExit(
                      f"Freshness/quality check failed: {ticker} invalid ordering "
                      f"(stop={stop}, entry={entry}, target={target})"
                  )
              key = (round(entry, 2), round(stop, 2), round(target, 2))
              duplicate_tuples.setdefault(key, []).append(str(p.get("ticker") or ""))
          dups = [f"{k}->{v}" for k, v in duplicate_tuples.items() if len(set(v)) > 1]
          if dups:
              raise SystemExit(f"Freshness/quality check failed: duplicate risk tuples across tickers: {dups[:3]}")

          print(f"Freshness check passed: engine={engine_name}, run_date={actual}")
          PY

      - name: Commit outputs to repo
        if: success()
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add outputs/
          if git diff --cached --quiet; then
            echo "No new outputs to commit"
          else
            DATE=$(date -u +%Y-%m-%d)
            git commit -m "auto: outputs for ${DATE}"
            ATTEMPTS=3
            PUSHED=0
            for i in $(seq 1 ${ATTEMPTS}); do
              if git push origin HEAD:main; then
                PUSHED=1
                echo "Outputs push succeeded on attempt ${i}/${ATTEMPTS}"
                break
              fi

              if [ "${i}" -lt "${ATTEMPTS}" ]; then
                echo "Push attempt ${i}/${ATTEMPTS} failed; rebasing on origin/main and retrying..."
                git fetch origin main
                if ! git rebase origin/main; then
                  echo "ERROR: Rebase failed while retrying outputs push"
                  git rebase --abort || true
                  exit 1
                fi
              fi
            done

            if [ "${PUSHED}" -ne 1 ]; then
              echo "ERROR: Failed to push outputs after ${ATTEMPTS} attempts"
              exit 1
            fi
          fi

      - name: Upload outputs artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: outputs
          path: outputs/**
          retention-days: 7
